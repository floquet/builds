Alan,
I'd like you to take a look at optimizing VASP on jade. I had a user request 2 new builds. They also said the existing binary was slower than at other sites. I rebuilt it months ago and found -fastsse caused it to crash. So it was built unoptimized. Alan Minga sent a list of options -fastsse includes. If I put the source in your workdir could you work through the options to find the one(s) that cause the code to fail. We may have to issue a PGI bug report on it.

I don't know how difficult it would be, but I'd like to know if it is possible to build it with PathScale which is often much faster than PGI.

Thanks,
Chris

 -------
Alan:

Ok, that's good. 

What we were finding was that the VASP ran significantly slower during one stage of the calculation - (i.e. subspace diagonalization of the orbitals). This is a Gramm Schmidt procedure that should scale fairly well if you compile with vender tuned libraries and something like scalapack.

Compared to other architectures Jade was very slow, even with scalapack.

If you try running the same job on say 8, 16, and 32 processors what kind of scaling do you get with the scalapack routines on? 

CHris
 -------
Alan:
THe problem children are captured by EDDIAG, and to a lesser extent ORTHCH.
THe "sub-space diag" is in EDDIAG.
CHris
 -------

Need to build for "gamma" and "scalapack"

 -lsci includes scalapack library and this is the default,
 but what in compile CPP options would result in using scalapack?

There is new ...36... version in gz.
Chris copied over the benchmark and potential files.

Scalapack can be switched off at run time, do manual.


Most calculations will be done in a work directory, and before starting
a calculation, several files must be created in this
directory.  The most important input files are:
INCAR POTCAR POSCAR KPOINTS

-------------

When the tar files are extracted, the directory name does not
include the third level of the version number.
Moreover, inside the distribution there is no indication of the
version, as far as I can find.
The files vasp.4.6.36.lib.tar.gz and vasp.4.6.36.tar.gz
become directories vasp.4.6 and vasp.4.lib .
I changed to include the subsubversion in order to avoid confusion.
The file vasp.4.6.36/README file
has as its latest entry the date 19.01.2007, which the 4.6.34
version lacks.  Both README files have the note
 > KNOWN BUGS:
 [...]
 > o TODO: ADDGRID: Fails with use of pgf compiler -> problem in us.F ?!?    

The documentation says to put the potentials in

VASP/pot/
VASP/pot_GGA/
VASP/potpaw/
VASP/potpaw_GGA/

and that can be done for the final installation.
However, because there are various versions and various
attempts at compilation I put the files in

VASP/Potentials/   [Precisely, /work/scheinin/VASP/Potentials/]
with subdirectory names
pot/  pot_GGA/  potpaw/  potpaw_GGA/  potpaw_PBE/

Note that Chris gave me five sets whereas the documentation
describes four sets.
The permissions were set so that everyone can read and
nobody can write.

I put the previous work in
/work/scheinin/VASP/Previous/
The first attempt at compiling will be in
VASP/PGI-noopt/

The first step is to compile in lib subdirectory.
The name of the library source directory was set to
src-vasp.4.6.36.lib whereas the installation directory
was set to vasp.4.6.36.lib.

The previous Makefile had
FC= ftn -Mx,119,0x200000
but I was not able to find -Mx in the PGI manual,
so I just used
FC= ftn 

For this initial test I used
CFLAGS = -fast
FFLAGS = -fast
For the optimized version it may be necessary to see what
specific option implied by
 -fastsse  results in an error and then eliminate just that
one [or few] option[s].

Ran "make" and "make install".

In the directory
/work/scheinin/VASP/PGI-noopt/vasp.4.6.36
a makefile.jade was created, which was then copied to Makefile.

In this subdirectory the PGI makefile explains
# the -Mx,119,0x200000 is required if you use older pgf90 versions

The CPP flags chosen are shown below

CPP    = $(CPP_) -DHOST=\"LinuxPgi\" \
          -Dkind8 -DscaLAPACK -DCACHE_SIZE=3000 -DMPI -DPGF90 

Note that  -DNGZhalf  is for the "gamma" version that is expected
to be 30-50 percent faster, but is not used here.

The previous Makefile had

LIB  = -L../vasp.4.6.34.lib -ldmy -lsci \
     ../vasp.4.6.34.lib/linpack_double.o  

however, the default ftn includes linking with -lsci.
The ERDC documentation says that ScaLAPACK is part of LibSci
which is automatically included when using Cray's default
compilers (ftn, cc).
HOWEVER, scalapack does not have some routines of linpack,
for example, it is missing zgedi, so in fact linpack_double.o
of vasp is needed.

I used

LIB  = -L../vasp.4.6.34.lib -ldmy

based on the assumption that linpack_double.o is only needed
if it cannot be found elsewhere.

Note that fftw was not used.
The previous Makefile had two definitions,
I used the second
FFT3D   = fft3dfurth.o fft3dlib.o
FFT3D   = fftmpi.o fftmpi_map.o fft3dlib.o 

In the makefile I dealt with SCA in the following way
# It is my hope and expectation that scalapack will be used and will be
# found in libsci.a
SCA=

To be consistent with previous work, an "install" target
was defined and the executable was given the name vasp-scalapack.

install:
	cp -f vasp-scalapack ../vasp.4.6.36-mpi/

Compiled and linked using
make vasp
make install

Executable is the following:
/work/scheinin/VASP/PGI-noopt/vasp.4.6.36-mpi/vasp-scalapack

Note that benchmark.tar.gz does not have a subdirectory.
Same problem with bench.Hg.tar.gz

mkdir benchmark-pgi-noopt
mkdir bench.Hg-pgi-noopt
cp benchmark.tar.gz benchmark-pgi-noopt
cp bench.Hg.tar.gz bench.Hg-pgi-noopt/
cd benchmark-pgi-noopt
tar xvzf benchmark.tar.gz
mkdir output

When running the benchmark, the executable indicated that
the version is "vasp.4.6.36 17Feb09 complex"
The job then printed
|      Recently Corning got a patent for the Teter Allan Payne algorithm      |
|      therefore VASP.4.5 does not support IALGO=8 any longer                 |
|      a much faster algorithm, IALGO=38, is  now implemented in VASP         |
|      this algorithm is a blocked Davidson like method and as reliable as    |
|      IALGO=8 used to be                                                     |
|      for ultimate performance IALGO=48 is still the method of choice        |
|      -- SO MUCH ABOUT PATENTS :)                                            |
|      ---->  I REFUSE TO CONTINUE WITH THIS SICK JOB ..., BYE!!! <----       |

and stopped.  So much for the user-friendly idea of providing
a way to check the compilation.
I changed the file INCAR to
   IALGO  =     48    algorithm
then the program ran.
The manual says
"If the output is correct, you might move to bench.Hg.tar" and the
manual shows the expected output.  The program ran but some values were
more than an order of magnitude different from the expected results.
The experiment was put into the file
benchmark-pgi-noopt.tar.gz
in order to save some useful files such as the batch submit file.
For bench.Hg the documentation says the time would be from 4-60
minutes, but for 16 processors the time was 23 seconds.

Now onto the optimized version.
The PGI manual says:
   Since release 7.0 -fast is synonymous with -fastsse;
therefore, both options include -Mvect=sse.

The optimizations used were
OFLAG  = -Minline -Mflushz -Mvect=sse -Mscalarsse -Mcache_align -Munroll -Mnoframe -Mlre -Mipa=fast


( Notes to myself, below
-fastsse is equal to -O2 -Munroll=c:1 -Mnoframe -Mlre (-fast) plus
-Mvect=sse, -Mscalarsse -Mcache_align,     -Mflushz

-Minline
-Mflushz
-Mvect=sse
-Mscalarsse
-Mcache_align
-Munroll
-Mnoframe
-Mlre
-Mipa=fast  need to both compile and link with this option
 Notes to myself, above )


 module switch PrgEnv-pgi  PrgEnv-pathscale

CFLAGS = -Ofast
FFLAGS = -Ofast
FREE   = -freeform

Error:
[2]: (/tmp/ulib/mpt/nightly/3.0/090808/mpich2/src/mpid/cray/src/adi/ptldev.c:191
0) PtlMDBind failed with error : PTL_PT_VAL_FAILED
2: user-supplied address and length: 0x2aaadf72d410 0x19e0 (6624 bytes)
2: Process VM layout:

 ======

OFLAG  =  -zerouv -O3 -ipa -fno-math-errno

[3]: (/tmp/ulib/mpt/nightly/3.0/090808/mpich2/src/mpid/cray/src/adi/ptldev.c:191
0) PtlMDBind failed with error : PTL_PT_VAL_FAILED
**** Segmentation fault!  Fault address: 0x2aab13aa8000
3: user-supplied address and length: 0x2aab137cc210 0x1f40 (8000 bytes)

 ======

CFLAGS = -O
FFLAGS = -O

lib-4205 : UNRECOVERABLE library error 
  Unable to find error message (check NLSPATH, file lib.cat)

 ======

Oh-zero
CFLAGS = -O0
FFLAGS = -O0

lib-4205 : UNRECOVERABLE library error 
  Unable to find error message (check NLSPATH, file lib.cat)


 ======


Google found:
2009-14-04 Compile instructions update
    Problems reported with VASP compiled using pathscale/3.2+xt-libsci/10.3.3+fftw/3.2 modules in louhi. Revert back to previous module versions of xt-libsci and fftw.
2008-11-24 Makefile update
    fixes a bug in wave.F and adds -DLONGCHAR to allow long lines in INCAR file
Another found with Google
The subroutine davidson.F must be handled with lower optimization setting
Add the following lines to the end of the VASP Makefile:
davidson.o : davidson.F
              $(CPP)
              $(FC)$(FFLAGS) -O1 -c $*$(SUFFIX)

Actually using:
  3) PrgEnv-pathscale/2.0.49
  5) xt-libsci/10.2.1
 

Using most of the options implied by -fastsse,
I created a VASP executable that had neither compilation errors
nor runtime errors.  It is between 10 to 20 percent faster
than the non-optimized version that I also compiled.  Both
versions (optimized and not optimized compiled by me) use scalapack.
The speedup is based on the lines in the file OUTCAR that begin
with "LOOP", for example,
       LOOP:  VPU time 1277.89: CPU time 1290.31
The optimizations used were
OFLAG  = -Minline -Mflushz -Mvect=sse -Mscalarsse \
 -Mcache_align -Munroll -Mnoframe -Mlre -Mipa=fast

With regard to the already installed vasp,
in /usr/local/applic/vasp/vasp.4.6.34-mpi/
there are both the executables runvasp and runvasp-scalapack.
Compared to the unoptimized version that I compiled (which uses
scalapack) vasp.4.6.34-mpi/runvasp is around 48 percent slower.
The speed of vasp.4.6.34-mpi/runvasp-scalapack was nearly equal
to the unoptimized version that I had compiled.

Chris, in order to further test the optimized version that I compiled,
which is the specific input data that resulted in an error?

The batch script files in ni_750 such as r_init and r_32 use
the executable
/usr/local/applic/vasp/vasp.4.6.34-mpi/runvasp
I wonder if the slow speed noted by the user woodward is due
to not using
/usr/local/applic/vasp/vasp.4.6.34-mpi/runvasp-scalapack.
Is there some reason for not using runvasp-scalapack?
Do the runvasp-scalapack results have an accuracy problem?

I was unable to compile VASP with the PathScale compiler.
Two tests with two choices for high-level optimization
"-Ofast" and then "-zerouv -O3 -ipa -fno-math-errno"
resulted in the runtime error
"PtlMDBind failed with error : PTL_PT_VAL_FAILED"
Two tests with two choices for low-level or no optimization
"-O" and then "-O0" (oh-zero) 
resulted in the runtime error
lib-4205 : UNRECOVERABLE library error 
  Unable to find error message (check NLSPATH, file lib.cat)

 ============
# The -DNGZhalf was mentioned in an example under the -DwNGZhalf
# section but only -DwNGZhalf was described as giving a speedup.  The
# choice of -DNGZhalf is said to reduce the storage requirement.
# For now, do not use the -DNGZhalf option. 
# -DNGZhalf
# -DwNGZhalf is the "gamma" option.
 -DwNGZhalf

 ============


 NPAR tells VASP to switch on parallelization over bands,
works wil RMM-DISS , which is ALGO=VERY_FAST
-DscaLAPACK
and LSCALAPACK=.TRUE.
but no value should also give this.
MPI_BLOCK  on XT3 choose LARGE blocks
NPAR between sqrt(p)*(0.5 to 1.0)
http://www.csc.fi/english/csc/courses/archive/material/fsd2007/presentations/ansaloni.pdf

For single image calculations, VASPâs parallelisation over plane waves and bands typically allows scaling up to 256 processors, depending upon the problem size. This dual level parallelisation can be controlled at runtime, allowing easy modification of the relative parallelisation of the main algorithms in the code. VASP makes heavy use of 3D parallel FFTs and of ScaLAPACKâs PDSYEV range of Eigensolvers, both of which have significant global communications overheads. Changing the relative parallelisation of these routines can minimise runtime to make good use of the available hardware. Despite VASPâs relatively poor scaling performance, its high floating point efficiency make this code a valuable tool.

looks like p above is number of CPUs.

As a matter of fact, it is of course possible to restart with an existing WAVECAR file even if the number of nodes has
changed. The only point that requires attention is that changing the NPAR parameter might also effect the number of
bands (NBANDS). WAVECAR files can only be read if the numbers of bands is strictly the same on the file and for the
present run. In some cases, it might be required to set the number of bands explicitly in the INCAR file by specifying
the NBANDS parameter

change that nw??? back to 3

for parallel version projected DOS is not evaluated if npar .ne. 1 and
lorbit 0-5 .

In VASP.4.5, the default for NPAR is equal to the (total number of nodes). For NPAR=(total number of nodes), each band
will be treated by only one node. This can improve the performance for platforms with a small communication bandwidth,
however it also increases the memory requirements considerably, because the non local projector functions must be stored in
that case on each node. In addition a lot of communication is required to orthogonalize the bands. If NPAR is neither 1, nor
equal to the number of nodes, the number of nodes working on one band is given by
total number nodes/NPAR.
The second switch which influences the data distribution is LPLANE. If LPLANE is set to .TRUE. in the INCAR file,
the data distribution in real space is done plane wise. Any combination of NPAR and LPLANE can be used. Generally,
LPLANE=.TRUE. reduces the communication band width during the FFT’s, but at the same time it unfortunately worsens
the load balancing on massively parallel machines. LPLANE=.TRUE. should only be used if NGZ is at least 3*(number of
nodes)/NPAR, and optimal load balancing is achieved if NGZ=n*NPAR, where n is an arbitrary integer. If LPLANE=.TRUE.
and if the real space projector functions (LREAL=.TRUE. or ON or AUTO) are used, it might be necessary to check the lines
following
real space projector functions
total allocation :
max/ min on nodes :
The max/ min values should not differ too much, otherwise the load balancing might worsen as well.

LINUX cluster linked by 100 Mbit Ethernet: On a LINUX cluster linked by a relatively slow network, LPLANE must
be set to .TRUE., and the NPAR flag should be equal to the number of nodes:
LPLANE = .TRUE.
NPAR = number of nodes.
LSCALU = .FALSE.
NSIM = 4
Mind that you need at least a 100 Mbit full duplex network, with a fast switch offering at least 2 Gbit switch capacity.
• T3D, T3E On many T3D, T3E platforms one is forced to use a huge number of nodes. In that case load balancing
problems and problems with the communication bandwidth are likely to be experienced. In addition the cache is fairly
small on T3E and T3D machines so that it is impossible to keep the real space projectors in the cache with any setting.
Therefore, we recommend to set NPAR on these machines to √ number of nodes (explicit timing can be helpful to find
the optimum value). The use of LPLANE = .TRUE. is only recommend if the number of nodes is significantly smaller
than NGX, NGY and NGZ.
In summary the following setting is recommended
LPLANE = .FALSE.
NPAR = sqrt(number of nodes)
NSIM = 1

File STOPCAR
LSTOP = .TRUE.
Above
temporary to have execution stop after first ionic step
No error when doing gamma when WAVECAR does not exist.
Job ran full hour during which time STOPCAR had not effect.
WAVECAR was created with zero size.

 ----


After a parallel run I had files
OSZ_init                   POTCAR
OUT_init
POSCAR

/bin/rm -f CHGCAR CHG WAVECAR CONTCAR DOSCAR TMPCAR vasprun.xml
/bin/rm -f EIGENVAL XDATCAR IBZKPT PCDAT
/bin/rm -f OUTCAR OSZICAR

Below, delete but not right away.
Maybe do a comparison of  OUTCAR  before deleting.
there is also OSZICAR.ref
Above, delete but not right away.

The files I got to start the run were

set W_DIR=/work/scheinin/VASP/Test/test-pgi-extreme-64/ni_750
set DATA_DIR=/work/scheinin/VASP/Test/Data/incars
cd $W_DIR
cp -f $DATA_DIR/incar_2.1 $W_DIR
# cp -f $DATA_DIR/incar_5 $W_DIR
cp -f $W_DIR/poscars/pos_tmp  $W_DIR/POSCAR
cp -f $W_DIR/incar_2.1 INCAR



Need 
INCAR  INCAR  KPOINTS  POSCAR  POTCAR 

--
Alter's file
#!/bin/ksh
#PBS -l ncpus=16
#PBS -l walltime=01:00:00
#PBS -N vasp_test
#PBS -q standard
#PBS -A myproject
#PBS -j oe

cd $PBS_O_WORKDIR
export VASP_LOC="/usr/local/applic/vasp"


rm -f  IBZKPT CHG DOSCAR OSZICAR PCDAT XDATCAR CONTCAR WAVECAR CHGCAR EIGENVAL O
UTCAR TMPCAR vasprun.xml

$VASP_LOC/vasp.4.6.34-mpi/runvasp 16 > vasp_mpi.out

echo " `grep '5 T=' vasp_mpi.out`   MPI version "


rm -f  IBZKPT CHG DOSCAR OSZICAR PCDAT XDATCAR CONTCAR WAVECAR CHGCAR EIGENVAL O
UTCAR TMPCAR vasprun.xml

$VASP_LOC/vasp.4.6.34/runvasp 1 > vasp_serial.out

echo " `grep '5 T=' vasp_serial.out`   SERIAL version "


echo " "
echo "  They should be very close to  "
echo "    5 T= 15635. E= -.39887404E+01 F= -.21616930E+02 E0= -.20775108E+02  EK
= 0.14147E+02 SP= 0.36E+00 SK= 0.31E+01   MPI version"
echo "    5 T= 15635. E= -.39890660E+01 F= -.21616865E+02 E0= -.20775612E+02  EK
= 0.14147E+02 SP= 0.36E+00 SK= 0.31E+01   SERIAL version"


rm -f  IBZKPT CHG DOSCAR OSZICAR PCDAT XDATCAR CONTCAR WAVECAR CHGCAR EIGENVAL O
UTCAR TMPCAR vasprun.xml 
vasp.pbs lines 1-33/34 99%

Batch work in:
/work/scheinin/VASP/Test/bench.Hg-pgi-all
Executables in /work/scheinin/VASP/Transfer
vasp.4.6.36:
vasp  vasp-gamma
vasp.4.6.36-mpi:
vasp-gamma  vasp-scalapack

|      Your FFT grids (NGX,NGY,NGZ) are not sufficient for an accurate        |
|      calculation.                                                           |
|      The results might be wrong                                             |
|      good settings for NGX NGY and  NGZ are                                 |
|                        31  31  and  31     


/work/usr_local/applic/vasp/vasp.4.6.36/vasp[-gamma]

#PBS -A erdcssta
#PBS -l ncpus=16
#PBS -l walltime=0:30:00
#PBS -q debug
#PBS -o /work/scheinin/VASP/Test/bench.Hg-pgi-all/output
#PBS -j oe
#PBS -N vasp
hostname
date

 -------------------

May 27, 2009
I have your code in place on jade and sapphire. I changed the
direcories slightly to keep the clutter of all these version
directories down. And David has a user asking for an even newer
version, so we'll be back on VASP at a later date.
Thanks,
Chris 

In fact, just one bin directory.

/usr/local/applic/vasp/4.6.36/bin
/usr/local/applic/vasp/4.6.36/bin/runvasp
/usr/local/applic/vasp/4.6.36/bin/runvasp-gamma
/usr/local/applic/vasp/4.6.36/bin/runvasp-gamma.orig
/usr/local/applic/vasp/4.6.36/bin/runvasp.orig
/usr/local/applic/vasp/4.6.36/bin/vasp
/usr/local/applic/vasp/4.6.36/bin/vasp-gamma
/usr/local/applic/vasp/4.6.36/bin/vasp-mpi-gamma
/usr/local/applic/vasp/4.6.36/bin/vasp-mpi-scalapack
/usr/local/applic/vasp/4.6.36/bin/runvasp-mpi
/usr/local/applic/vasp/4.6.36/bin/runvasp-mpi-gamma
/usr/local/applic/vasp/4.6.36/bin/runvasp-mpi-gamma.orig
/usr/local/applic/vasp/4.6.36/bin/runvasp-mpi.orig
/usr/local/applic/vasp/4.6.36/bin/vasp-mpi
/usr/local/applic/vasp/4.6.36/bin/runvasp-mpi-scalapack
/usr/local/applic/vasp/4.6.36/bin/runvasp-mpi-scalapack.orig
/usr/local/applic/vasp/4.6.36/lib
/usr/local/applic/vasp/4.6.36/lib/lapack_atlas.o
/usr/local/applic/vasp/4.6.36/lib/libdmy.a
/usr/local/applic/vasp/4.6.36/lib/linpack_double.o

