\documentclass[12pt]{article}
% Turn on geometry when draft is turned-off
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry} 
\usepackage{mystuff}

%\usepackage[draft]{pdfdraftcopy}

%\draftstring{DRAFT}
%\draftfontsize{150}
%\draftangle{45}
%\definecolor{mydraftcolor}{rgb}{0.85,0.85,0.95}
%\draftcolor{mydraftcolor}

\renewcommand{\baselinestretch}{1.1}

\begin{document}
\pagenumbering{arabic}

\subsection*{Amount of Disk Space}
\label{subsec:amount-of-disk-space}

The total size of the dataset needed for checkpointing
was determined for four applications.
In addition, the time needed to write the files was measured.
The checkpoint dataset is written to the directory
{\tt /work/PBS/cpr/<jobid>.pbs.CK}.
The command ``{\tt du -sm *}'' was used to determine the dataset size.
During early tests it was noted that execution does not pause
if the time needed for writing the checkpoint dataset exceeds
five minutes.
Though attempts were made by the ERDC Cray Support Staff to solve
this problem, during the final week of testing the problem remained.
In Table~\ref{tab:time-dataset-size} the dataset size is shown
in parathesis when the write time was greater than five minutes,
since it is not known whether the size would be the same if
CPR worked correctly.
The table entry ``$ > 300 $'' denotes CPR failure
because the job continued to run
rather than going into the ``hold'' state.
For jobs that were successfully put into the ``hold'' state
({\tt qhold} $<$jobid$>$) then released ({\tt qrls} $<$jobid$>$),
and also for jobs that continued to run
despite use of the {\tt qhold} command,
the program results were correct when the run finished.
Moreover, for every test, whether successful or not,
the directory containing the checkpoint dataset
was automatically deleted when the job finished.
Table~\ref{tab:time-dataset-size} is correct
in showing that the case of CTH with 64 processes
required more time for writing the checkpoint dataset than
CTH with 288 processes.

\begin{table}[th]
\vspace*{2ex}
\begin{center}
\begin{tabular}{|l|r|r|r|} \hline
Application & Num. Procs. &
 Checkpoint Write Time (secs) & Checkpoint Dataset Size (MB) \\
\hline
CTH     &   64       & $>$ 300    &   (26663)       \\
CTH     &  288       &     167    &    55041  $\:$  \\
OOCORE  &   64       &     125    &    43351  $\:$  \\
OOCORE  &  288       &     122    &    34955  $\:$  \\
HYCOM   &   59       &      80    &    26239  $\:$  \\
HYCOM   &  256       &     283    &    93783  $\:$  \\
WRF     &  256       &      75    &    24205  $\:$  \\
\hline
\end{tabular}
\end{center}
\caption[Checkpointing Time and Dataset Size.]{
{\bf Checkpointing Time and Dataset Size.}
This table can be used for projecting the amount of disk space
needed for a large job on a machine such as jade.
On jade each node has 8 GBytes of memory and 4 cores per node,
so for a program that fully utilizes memory,
if it runs on 1000 cores,
then 2 TeraBytes of memory would be needed for the checkpoint dataset.
In practice, the worst case for these tests was HYCOM
for which the total size of the checkpoint files for a run
with 1000 cores can be projected to be 0.37 TeraBytes.
}
\vspace*{0.5cm}
\label{tab:time-dataset-size}
\end{table}

\clearpage

\end{document}
