
First Test
----------

The first test was small Fortran program that passed an integer
between processes. The first process passes an integer to the
second process using MPI_Send() and MPI_Recv().  The second process
adds one and passes the integer to the next process.  The last
process adds one and passes it to the first process.  In addition,
each process writes to a file the number it passed.  An iteration
count of 4194304 cycles resulted in a program that ran for about five
minutes, which gave enough time to checkpoint the program by hand.
A processes count of 64 was tested.  The output file was checked
using a Perl program.

The use of MPI was in a separate file, as was the output of writing
to a file.  This resulted in three object files: the main program,
the MPI part and the file output part.  The latter two files were
compiled with and without loading the module for checkpointing, blcr.
The goal was to demonstrate that libraries compiled without
checkpointing could be used in a checkpointed program as long as the
linking was done with the blcr module loaded.  This proved to be the
case, that the object files for MPI and I/O did not need to be compiled
with checkpointing.

Requirements of CPR
-------------------

Cray XT System Software 2.2 Release Overview (S-2425-22)
describes the requirements and limitations
of checkpoint/restart.
scheinin:onyx01% S%GÃ¢Â€Â“%@2425%GÃ¢Â€Â“%@22

2.3.1 Requirements and/or Limitations for Checkpoint/Restart

Specific third-party batch system software releases are required for CPR
support.
For more information, access the 3rd Party Batch SW link on the CrayPort
website at crayport.cray.com.
For information about accessing CrayPort, see CrayPort.

CPR applications on Cray XT systems require a library
that has integrated BLCR support; for that reason,
applications must be linked with the Cray MPT 3.0.1 release or
later libraries.
Thus, only applications using the MPI and SHMEM programming models
are checkpointable.

For performance monitoring of applications that may be checkpointed
and restarted, CrayPat 4.4 release is also required.




On onyx01.
/work/scheinin/CTH/noCPR/ssp_test
% module list
Currently Loaded Modulefiles:
  1) modules/3.1.6
  2) cray/MySQL/5.0.64-1.0000.2342.16.1
  3) java/jdk1.6.0_17
  4) pbs/10.2.0.93147
  5) xtpe-target-cnl
  6) xt-service/2.2.41A_PS05
  7) xt-os/2.2.41A_PS05
  8) xt-boot/2.2.41A_PS05
  9) xt-lustre-ss/2.2.41A_PS05_1.6.5
 10) cray/job/1.5.5-0.1_2.0202.18632.46.1
 11) cray/csa/3.0.0-1_2.0202.18623.63.1
 12) cray/account/1.0.0-2.0202.18612.42.3
 13) cray/projdb/1.0.0-1.0202.18638.45.1
 14) Base-opts/2.2.41A_PS05
 15) pgi/9.0.4
 16) xt-libsci/10.4.0
 17) xt-mpt/3.5.0
 18) xt-pe/2.2.41A_PS05
 19) xt-asyncpe/3.3
 20) PrgEnv-pgi/2.2.41A_PS05

./1_BUILD_COMPILE_SSPs.sh
./2_CHECK_BINS.sh


On onyx01.
/work/scheinin/CTH/wtCPR1/ssp_test
module load blcr
% module list
Currently Loaded Modulefiles:
  1) modules/3.1.6
  2) cray/MySQL/5.0.64-1.0000.2342.16.1
  3) java/jdk1.6.0_17
  4) pbs/10.2.0.93147
  5) xtpe-target-cnl
  6) xt-service/2.2.41A_PS05
  7) xt-os/2.2.41A_PS05
  8) xt-boot/2.2.41A_PS05
  9) xt-lustre-ss/2.2.41A_PS05_1.6.5
 10) cray/job/1.5.5-0.1_2.0202.18632.46.1
 11) cray/csa/3.0.0-1_2.0202.18623.63.1
 12) cray/account/1.0.0-2.0202.18612.42.3
 13) cray/projdb/1.0.0-1.0202.18638.45.1
 14) Base-opts/2.2.41A_PS05
 15) pgi/9.0.4
 16) xt-libsci/10.4.0
 17) xt-mpt/3.5.0
 18) xt-pe/2.2.41A_PS05
 19) xt-asyncpe/3.3
 20) PrgEnv-pgi/2.2.41A_PS05
 21) blcr/0.7.3

./1_BUILD_COMPILE_SSPs.sh
./2_CHECK_BINS.sh

On onyx01.
/work/scheinin/CTH/wtCPR2/ssp_test
module load blcr


On onyx01.
/work/scheinin/CTH/wtCPR3/ssp_test
module load blcr


Remaining questions.
Is it necessary to load the blcr module in the batch script?

Remember to load blcr for wtCPR# batch scripts.

Have noCPR and wtCPR1 run without stopping and check that there
are no other jobs on the machine.  In this way the speed
will be tested.

Need to generate directory name 0288 and need to change
file names that have 384 in the name.

./build_cth  There is a trick uname that give machine name jade.
./4_CHECK_Valid.sh  Done, skip check of large.
./3_SUBMIT_ssp.sh  Done.
./ded/cth/large/0384/cth_large_0384.sub  Done.
./ded/cth/large/0384/cth_large_0384.bat  Done.
./5_GET_Walltimes.sh  Done

./app/cth/README  as an explanation
./ded/cth/large/ref/cth_lrg_0384.o88942  as an explanation
./ded/cth/large/ref/octh_Large  no longer a useful reference
./ded/cth/large/ref/ogcth_Large  no longer a useful reference

Need to modify aprun command Check that no change for simple hold by hand and so need to change for checkpointing automatically.

Need to test restarting by hand job checkpointed automatically after it has been deleted.  Is there just one version saved?  But to be more precise, deleting by hand with qdel should erase the job forever (?).  Need to try starting job by hand after the job dies for some other reason.

 ---

/work/PBS/cpr  look at usage 

Let run for 5 min, stop-start run for 5 stop-start run 5 stop-start run to end

write how long the hold and the release take to complete.

After hold paused for awhile, wrote
qhold: No such file or directory 5584.pbs
and job continued to run.

64 processes used 28.6 GB and took 5 min 14 seconds to write files.

qhold
qrls

qhold 5584
qrls 5584
du -sm /work/PBS/cpr/5584.pbs.CK
For 64 nodes, above.

man qsub says
 -v asdlfkj , alaksjdf
but does not work, neither does
 -v asdlfkj -v alaksjdf
What seems to work is
 -v asdlfkj,alaksjdf

./cth_standard_0064.sub
5585.pbs

Marty,

I've begun the CPR tests that I've been requested to do on onyx.
I'm running CTH with 64 processes.
The submit script is 

/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/standard/0064/cth_standard_0064.sub
and the batch script is
/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/standard/0064/cth_standard_0064.bat

After four minutes I did

qhold 5585

and the command does not return while /work/PBS/cpr/5585.pbs.CK grows and grows, which I assume is perfectly normal.  During this time the job remains in state "R".

I'm sure (95% or better) that the executable was compiled after the blcr module was loaded.

Now comes the odd part.  After 5 minutes and 14 seconds the qhold 5585 command returns with

"qhold: No such file or directory 5585.pbs"

The space used for checkpointing is

scheinin:onyx01% du -sm /work/PBS/cpr/5585.pbs.CK
28658   /work/PBS/cpr/5585.pbs.CK

The status of the job is still at "R"

scheinin:onyx01% qstat -u scheinin
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5585.pbs        scheinin standard cth_std_00   7701   1   1    --  01:00 R 00:12

Minute after minute the job continues to run, it never goes into a hold state.
There is further strangeness.  The job should finish after 39 minutes but after 58 minutes the job is still running, at least it is still shown as being in "R" state.

I was planning on trying qrls to see what happens but the job exceeded the 1 hour time limit set in the script before I had a chance.  The contents of /work/PBS/cpr was cleaned-up automatically.

Alan

Similar to above, trying again after queue fixed
/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/standard/0064


qhold 5595  around 11:21
/work/PBS/cpr/5595.pbs.CK/00000001 is created but remains empty
qrls 5595


 ------------------------------

 setenv NETCDF /work/scheinin/WRF/netcdf/netcdf-4.0.1-cnl
 setenv PNETCDF /work/scheinin/WRF/pnetcdf/pnetcdf-1.1.0
 setenv PHDF5 /work/scheinin/WRF/hdf5/1.8.4-cnl

 setenv WRFIO_NCD_LARGE_FILE_SUPPORT 1

  24.  Cray XT CNL/Linux x86_64, PGI compilers  (dm+sm)
1 basic nesting

modify configure.wrf

 ./compile wrf | & tee listing  

 wrf/external/io_netcdf
 It is important that libwrfio_nf.a be built and also
 that the executable diffwrf be built.

Working on noCPR,
when doing others, after generating configure.wrf then
copy it from noCPR

wrf_std_00128.bat
wrf_std_00256.bat

copy above from noCPR because account, queue and time was fixed
but maybe wait until wtCPR1 where CPR module is added

for wtCPR# need to change *.bat 

/work/scheinin/WRF/noCPR/WRFV3.1.1_bench/wrf/standard/00128
./wrf_std_00128.sub
Job waits in queue

Added a paragraph for onxy that uses already existing directory jade

directory /work/scheinin/SSP
subdirectories
noCPR  wtCPR1  wtCPR2  wtCPR3
untar'ed ssp_hycom_oocore.tar
and copied to each the modified
build_hycom and build_oocore

In noCPR, build seemed to go O.K. except
oocore error
cp: cannot stat `submit/jade/testdriver.in-std_0064': No such file or directory

Need to change ded/<hycom|oocore>/large/0385 to something a bit smaller,
e.g. 288.

 ==================

Alan,

The default timeout limit for the checkpoint action was set to 300 seconds (5 minutes).  Since we are currently checkpointing to a single OST, the checkpoint action is expected to be slower than desired.  Once the pbs_mom daemon hit the wdog limit, it aborted the checkpoing and sent a sigkill to the job.  Unfortunately, the results were clearly unpredictable in that the job was not removed from the queue even though the associated aprun was killed.

I have reset the parameter to be unlimited while we continue testing.  This is something that we will need to revisit before going into production in order to decide on a more suitable time period.

The timeout for the checkpoint_abort action is still set to 300 seconds.  This is the action that pbs_mom performs to actually terminate the job after the checkpoint action completes with status 0 (if the checkpoint action returns any other status, pbs_mom will send a sigkill to the job).

The checkpoint_restart action is configured to start in the background (to avoid blocking pbs_mom) and does not monitor the time required to restart.

You should be able to continue running your tests.  While doing so, could you please document the times required for various sizes of jobs?  You can do this via a date command before and after the qhold or just timing the command.  I expect the times to be linear, but would be interested in your results for comparison to future testing with different stripe configurations.

Thanks,

--Marty

 ==================

move to more relevant part of this document

/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/standard/0064
starting  Wed Mar 24 09:15:04 CDT 2010  (after 4 minutes of run time)
finished around Wed Mar 24 09:18:52 CDT 2010 (or just a little less)
% du -sm /work/PBS/cpr/5596.pbs.CK
28963   /work/PBS/cpr/5596.pbs.CK
job shows stat of H

qrls 5596
starting to run Wed Mar 24 09:22:22 CDT 2010

/work/PBS/cpr/5596.pbs.CK still there

six minutes later
qhold 5596  Wed Mar 24 09:25:03 CDT 2010

1% du -sm *
10249   5596.pbs.CK
28963   5596.pbs.CK.old

 finished holding at Wed Mar 24 09:28:34 CDT 2010

qstat elapsed time 19 minutes (including file writing time)

1% du -sm *
28918   5596.pbs.CK
1       cprerror.log
second qrls 5596 resulted in state "Q"

What happens if checkpointing time cycle is less than needed?

 --------------------
It looks like there were 4 reservations from the previous failures that were not properly cleaned up and were holding resources in ALPS.

I've canceled those reservations and started 5596.

--M.

Need to test clean-up problem.

 --------------------

% pwd
/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/standard/0064

% date
Wed Mar 24 14:03:58 CDT 2010
% qhold 5598 ; date
Wed Mar 24 14:07:55 CDT 2010

qrls 5598

waited 5 minutes in case startup takes time,
though "R" state starts immediately.

% date
Wed Mar 24 14:16:31 CDT 2010
% qhold 5598 ; date
Wed Mar 24 14:20:35 CDT 2010

% du -sm *
5557    5598.pbs.CK
30850   5598.pbs.CK.old

% du -sm *
30711   5598.pbs.CK

qrls 5598
job running now, in "R" state rather than "Q"

date ; qhold 5598 ; date

% date ; qhold 5598 ; date
Wed Mar 24 14:30:38 CDT 2010
Wed Mar 24 14:34:39 CDT 2010
job running now, in "R" state rather than "Q"

/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/standard/0064/cth_standard_0064.bat
walltime limit of one hour is too short
#PBS -l walltime=01:00:00

Cleanup went OK.
% du -sm *
1       cprerror.log

/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/large/0288

after two minutes

date ; qhold 5603 ; date

cd /work/PBS/cpr
du -sm *

-----------------------------

OOCORE

cd /work/scheinin/SSP/{no|wt}CPR[1|2|3]/app/oocore/bin/build
make a subdirectory onxy that has the same files as jade

File/work/scheinin/SSP/noCPR/build_oocore
made a case onxy* that is identical to jade*
and copied it to wtCPR{1|2|3}

cd /work/scheinin/SSP/noCPR
./build_oocore

For wtCPR# cases,
module load blcr/0.7.3
cd /work/scheinin/SSP/wtCPR{1|2|3}
./build_oocore

Missing files (inp/testdriver.in) so that the final steps,
shown below, of build_oocore are not done.
    cd $CWD/ded/oocore/standard
    cp submit/jade/testdriver.in-std_0064 inp/testdriver.in
    cd $CWD/ded/oocore/large
    cp submit/jade/testdriver.in-lrg_0384 inp/testdriver.in

Created directories
/work/scheinin/SSP/{no|wt}CPR[1|2|3]/ded/oocore/large/0288



scheinin:onyx01% find /work/scheinin/SSP/noCPR -name testdriver.in
/work/scheinin/SSP/noCPR/app/oocore/src/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/inp/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/ref/0192/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/ref/0384/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/ref/0256/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/ref/0128/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/ref/0512/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/large/ref/1024/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/standard/inp/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/standard/ref/32_bit_output/48/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/standard/ref/32_bit_output/96/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/standard/ref/32_bit_output/32/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/standard/ref/32_bit_output/128/testdriver.in
/work/scheinin/SSP/noCPR/ded/oocore/standard/ref/32_bit_output/64/testdriver.in

But testdriver.in does exist as shown above
It seems to actually be an output file generated by the run.

/work/scheinin/SSP/noCPR/ded/oocore/large/0288
already have
oocore_large_0288.bat  oocore_large_0288.sub
but I need to see if they are correct

cd /work/scheinin/SSP/noCPR/ded/oocore/standard/0064
 ./oocore_standard_0064.sub
pbs: 
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5605.pbs        scheinin standard oocore_std    --    1   1    --  01:00 Q   -- 


Both files  oocore_standard_0064.o5605 and oocore_std_0064.o5605 are generated,
why both.  Also testdriver.in and valid.o5605 are generated.

Location of executable needs to be changed in *.bat file.
aprun: file /work/scheinin/ssp_test/app/oocore/bin/testzdriver-std not found

may need to change way of submitting job, to pass CPR_ENV

do noCPR and wtCPR1 64-process cases and compare times.
Then setup directories for other 64-process cases and 228 process cases.
(For example /work/scheinin/SSP/wtCPR1/ded/oocore/large/0288/)


/work/scheinin/SSP/wtCPR{1|2|3}/ded/oocore/standard/0064/oocore_standard_0064.sub
change line to
qsub -v $CPR_ENV oocore_standard_0064.bat

For 0288 need to do above and probably much more.

So far, just did wtCPR1 and do others when I see how things run.

Make sure for each *.bat see that there is a ../inp/testdriver.in
and modify BINDIR 

binroot=`pwd | sed -e "s?\(.*\)/ded/oocore.*?\1?"`
BINDIR=${binroot}/app/oocore/bin
or some either executabel

time limit needs to be increased

 --------
hycom

cd /work/scheinin/SSP/noCPR
./build_hycom

ran without error, need to see where new files ended up
mv hycom ../bin/hycom.standard
mv hycom ../bin/hycom.large

 ---------------

One of the CTH (?) scripts has the wrong way of specifying
environment variables to pass
What seems to work is
 -v asdlfkj,alaksjdf
but the script has -v abc -v def

 ---------------

In report add Marty's comment and try myself
 "it doesn't seem to like a hold combined with a scheduled checkpoint"

 ---------------

/work/scheinin/SSP/wtCPR1/ded/oocore/standard/0064
module load blcr
module list  includes  blcr/0.7.3

 -----------------

Tom Oppe and Carrie Leach are our HYCOM experts.

The input files *depth.* and *grid.*, nor the forcing* and blkdat.input are specific to the number of cores used.  The patch.input files are specific, however.

To reduce the number of processors, you might need to recompile the HYCOM binary.  It depends on the sizes of the parameters idm and jdm in dimensions.h_large.  Processor counts 64, 123, 256, 385, 504, 625, 766, 882, 1006, 1267, 1788, 2040, 2559, 3031, 4086, and 5107 are supported.  The number you cite is at the lower end of this range.  Also, and importantly, only these processor counts are supported.  Any others will generate error and diagnostic messages that indicate the number of MPI processes does not match the requirements of idm and jdm.

 -----------------
cd /work/scheinin/SSP/noCPR/app/hycom/src
OR
cd /work/scheinin/SSP/wtCPR1/app/hycom/src
vi dimensions.h_large 
chose
      parameter (idm= 225,jdm= 207)  ! NMPI=256
for dimensions.h_standard left as
      parameter (idm= 300,jdm= 275)  ! NMPI=24,47,59,80,96,111,124

cd /work/scheinin/SSP/noCPR/app/hycom/bin/build/jade
OR
cd /work/scheinin/SSP/wtCPR1/app/hycom/bin/build/jade

xt4_mpi OK as is, for larger array sizes the compilation fails
but attempts at adding
  -fPIC -shared [-Mlarge_arrays] [-mcmodel=medium] to FCFFLAGS resulted in
a segmentation fault at runtime.

If there is a build error, may need to do
cd /work/scheinin/SSP/noCPR/app/hycom/src
OR
cd /work/scheinin/SSP/wtCPR1/app/hycom/src
rm -f *.o
The build_hycom command starts from scratch -- almost -- but does not clean-up
certain object files.

Need to load even if not used
module load fftw/3.2.2.1

./build_hycom

A 0385 directory will be built that must be copied and modified
to a 0256 directory.  Cannot use 287 because there is no patch.input
file available for 287.

Above in /work/scheinin/SSP/noCPR without blrc and
above in /work/scheinin/SSP/wtCPR1 WITH  blrc (module load blcr/0.7.3)

In /work/scheinin/SSP/noCPR/ded/hycom/large
or /work/scheinin/SSP/wtCPR1/ded/hycom/large

copy directory 0385 to 0256 and modify everywhere that 0385 is used
in file names and inside files.

In *.bat file change from
export P=ssp_test/ded/hycom/large
change to
export P=SSP/noCPR/ded/hycom/large
(or wtCPR1) as appropriate
and
from
/bin/cp ${WORKDIR}/ssp_test/app/hycom/bin/hycom.large ./hycom
to
/bin/cp ${WORKDIR}/SSP/noCPR/app/hycom/bin/hycom.large ./hycom

 ---------
Results in 
wtCPR1/ded/hycom/large/0256_saveme
wtCPR1/ded/hycom/standard/0059_saveme

copied wtCPR1 to wtCPR2 and wtCPR3
changed wtCPRn inside the files
wtCPR2/ded/hycom/large/0256/hycom_large_0256.bat
wtCPR2/ded/hycom/large/0256_saveme/hycom_large_0256.bat
wtCPR2/ded/hycom/standard/0059/hycom_standard_0059.bat
wtCPR2/ded/hycom/standard/0059_saveme/hycom_standard_0059.bat
idem for wtCPR3

--------

scheinin:onyx01% pwd
/work/scheinin/SSP/wtCPR2/ded/hycom/standard/0059

date ; qhold 5670 ; date
Wed Mar 24 14:07:55 CDT 2010

qrls 5670

waited 3 minutes in case startup takes time,
though "R" state starts immediately.

then again


cd /work/PBS/cpr

du -sm * ; date

1% date ; qhold 5670 ; date
Thu Apr 29 12:21:30 CDT 2010
Thu Apr 29 12:22:54 CDT 2010

% 
scheinin:onyx01% du -sm * ; date
1       cprerror.log
Thu Apr 29 12:17:37 CDT 2010
scheinin:onyx01% du -sm * ; date
950     5670.pbs.CK
1       cprerror.log
Thu Apr 29 12:21:39 CDT 2010
scheinin:onyx01% du -sm * ; date
26195   5670.pbs.CK
1       cprerror.log
Thu Apr 29 12:24:24 CDT 2010

% qstat -u scheinin
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5670.pbs        scheinin standard hycom_std_  23541   1   1    --  01:00 H 00:06


% qrls 5670
5670.pbs        scheinin standard hycom_std_  23860   1   1    --  01:00 R 00:091
% du -sm * ; date
26195   5670.pbs.CK
1       cprerror.log
Thu Apr 29 12:26:47 CDT 2010

% du -sm * ; date
1       5670.pbs.CK
26195   5670.pbs.CK.old
1       cprerror.log

Second qhold failed:

scheinin:onyx01% qhold 5670
qhold: Request invalid for state of job 5670.pbs

At the time the state of the job was
scheinin:onyx01% qstat -u scheinin
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5670.pbs        scheinin standard hycom_std_  23860   1   1    --  01:00 R 00:13

The backup files show the following

scheinin:onyx01% pwd
/work/PBS/cpr
scheinin:onyx01%  du -sm * ; date
1       5670.pbs.CK
26195   5670.pbs.CK.old
1       cprerror.log
Thu Apr 29 12:37:05 CDT 2010

At end of run the results were declared valid.

% pwd
/work/scheinin/SSP/wtCPR2/ded/hycom/large/0256

date ; qhold 5671 ; date

scheinin:onyx01% date ; qhold 5671 ; date
Thu Apr 29 12:58:23 CDT 2010
Thu Apr 29 13:03:16 CDT 2010

did not go past the 5 minute mark.

qrls 5671

waited 3 minutes in case startup takes time,
though "R" state starts immediately.

then again

cd /work/PBS/cpr

du -sm * ; date

scheinin:onyx01% du -sm * ; date
2810    5671.pbs.CK
1       cprerror.log
Thu Apr 29 12:58:36 CDT 2010
scheinin:onyx01% du -sm * ; date
66209   5671.pbs.CK
1       cprerror.log
Thu Apr 29 13:01:31 CDT 2010
scheinin:onyx01% du -sm * ; date
97883   5671.pbs.CK
1       cprerror.log
Thu Apr 29 13:05:01 CDT 2010

qrls 5671
scheinin:onyx01% du -sm * ; date
97883   5671.pbs.CK
1       cprerror.log
Thu Apr 29 13:05:49 CDT 2010

-----
scheinin:onyx01% qrls 5671
scheinin:onyx01% qstat -u scheinin

pbs: 
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5671.pbs        scheinin standard hycom_lrg_  24634   1   1    --  01:00 R 00:27
scheinin:onyx01% date ; qhold 5671 ; date
Thu Apr 29 13:22:30 CDT 2010
qhold: No such file or directory 5671.pbs
Thu Apr 29 13:22:31 CDT 2010
scheinin:onyx01% qstat -u scheinin

pbs: 
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5671.pbs        scheinin standard hycom_lrg_  24634   1   1    --  01:00 R 00:29
scheinin:onyx01% 

scheinin:onyx01% du -sm * ; date
1       5671.pbs.CK
97883   5671.pbs.CK.old
1       cprerror.log
Thu Apr 29 13:23:05 CDT 2010



Assume that the program has a way of determining by itself that
it is not working properly.

Compile with -Mtraceback  and  -g

In a Fortran routine that determines it is time to abort
in preamble

      use DFPORT

and have a line with

      abort().

Before starting the program set environment variable

export PGI_TERM=option,option

[no]debug    debugging invoked on error
[no[trace    traceback on error
[no]signal   establishment of signal handlers for errors
[no]abort    enables/disables calling abort() that creates a core file.

A trace might be enough, if want a core dump, make sure one is allowed

Using csh:
% limit coredumpsize unlimited
% setenv PGI_TERM abort
Using bash, sh, zsh, or ksh:
$ ulimit -c unlimited
$ export PGI_TERM=abort

To debug a core file with pgdbg, start pgdbg with the -core option.
For example, to view a core file named
"core" for a program named "a.out":
$ pgdbg -core core a.out


From Alan Minga below.
 ---------------------------
I think that's right.  

> -----Original Message-----
> From: Scheinine, Alan ERDC-ITL-MS Contractor 
> [mailto:Alan.Scheinine@usace.army.mil]
> Sent: Tuesday, February 23, 2010 1:44 PM
> To: Alan Minga; Bullock, Marty W ERDC-ITL-MS Contractor; Duckworth, 
> Richard J ERDC-ITL-MS
> Subject: building for checkpointing
> 
> I want to thank everyone for the notes on checkpointing (CPR).
> 
> I need to be 100 percent certain that I understand how to build for 
> checkpointing CPR.
> Is it true that applications do NOT NEED to be built (compiled and
> linked)
> while the "blcr" module is loaded?  The only requirement is that the 
> modules be built with MPT 3.0.1 or later?
> 
> In other words, on sapphire and jade libaries in /usr/local/usp or 
> $PET_HOME that have been built with MPT before we had the blcr module, 
> can be used with CPR as long as the job is submitted (qsub) after the 
> blcr module is loaded?
> [And, of course, qsub -v $CPR_ENV <jobscript> [? and "module load blcr"
> in
> the batch script ?]]
> 
> With regard to the comments I put into square brackets, to be more 
> precise, I put "module load blcr" in the batch script just to be sure, 
> I don't know if it is necessary.  Also, I put "setenv LD_PRELOAD 
> /usr/lib64/libcr_run.so.0"
> in the batch script rather than "qsub -v $CPR_ENV" and that did not 
> work.
> Apparently, "qsub -v $CPR_ENV" is necessary and setting LD_PRELOAD 
> within the batch script is no substitute.
> 
> Alan
 -------------
Here's the current list from the craydocs website


There are others like you can't be guarntted a process geometry on restart.  You'll get the same # of nodes back but you won't get the same nodes.

Task layout across the nodes can change.

2.3.1 Requirements and/or Limitations for Checkpoint/Restart

Specific third-party batch system software releases are required for CPR support. For more information, access the 3rd Party Batch SW link on the CrayPort website at crayport.cray.com. For information about accessing CrayPort, see CrayPort.

CPR applications on Cray XT systems require a library that has integrated BLCR support; for that reason, applications must be linked with the Cray MPT 3.0.1 release or later libraries. Thus, only applications using the MPI and SHMEM programming models are checkpointable.

For performance monitoring of applications that may be checkpointed and restarted, CrayPat 4.4 release is also required.

Due to the known file-per-node I/O access of checkpoint/restart, the checkpoint directory's file system setting should be optimized for this access pattern. For Lustre, it is optimal to set the checkpoint directory stripe count to one.

lfs setstripe checkpoint_dir -s 0 -i -1 -c 1

> -----Original Message-----
> From: Scheinine, Alan ERDC-ITL-MS Contractor 
> [mailto:Alan.Scheinine@usace.army.mil]
> Sent: Wednesday, February 24, 2010 4:15 PM
> To: Alan Minga
> Subject: RE: building for checkpointing
> 
> Alan,
>    I'm compiling gamess on diamond for both sockets and shmem, which 
> got me thinking.  Leaving aside diamond, focusing on a Cray computer, 
> if I want to use CPR, what are the limits?  I have a Cc in which you 
> wrote to Bob Maier that "The limitations of CPR  are documented in the 
> 2.2 SW release."
> Where
> would I find the document that describes the limits of CPR?
> Alan
From Alan Minga above.

 ----------------
The diff -y

/work/scheinin/CTH/noCPR/ssp_test/ded/cth/large/0288

shows to non-CPR test cases and that a few numerical differences
are to be expected.

CPR
1080c1080
<   (  1, 0.000000E+00) (  2, 2.098458E-10) (  3, 4.727158E+00) (  4, 1.979670E-10) (  5, 2.039064E-10) (  6, 9.898348E-09) 
---
>   (  1, 0.000000E+00) (  2, 2.098458E-10) (  3, 6.408525E+00) (  4, 1.979670E-10) (  5, 2.039064E-10) (  6, 9.898348E-09) 
1361c1361
<    CPUTOT =  4.02808E+03
---
>    CPUTOT =  4.03172E+03
1386c1386
<   (  1, 1.001070E-06) (  2, 6.313221E-09) (  3, 4.028085E+03) (  4, 6.684358E-09) (  5, 6.498790E-09) (  6, 6.313221E-09) 
---
>   (  1, 1.001070E-06) (  2, 6.313221E-09) (  3, 4.031720E+03) (  4, 6.684358E-09) (  5, 6.4

cth_lrg_0288.o5582:++++ SSP-07 ++++ cth_large_0288 walltime: 4079 seconds
cth_lrg_0288.o5695:++++ SSP-07 ++++ cth_large_0288 walltime: 4084 seconds

noCPR
1080c1080
<   (  1, 0.000000E+00) (  2, 2.098458E-10) (  3, 5.332589E+00) (  4, 1.979670E-10) (  5, 2.039064E-10) (  6, 9.898348E-09) 
---
>   (  1, 0.000000E+00) (  2, 2.098458E-10) (  3, 6.400658E+00) (  4, 1.979670E-10) (  5, 2.039064E-10) (  6, 9.898348E-09) 
1361c1361
<    CPUTOT =  4.02215E+03
---
>    CPUTOT =  4.02300E+03
1386c1386
<   (  1, 1.001070E-06) (  2, 6.313221E-09) (  3, 4.022155E+03) (  4, 6.684358E-09) (  5, 6.498790E-09) (  6, 6.313221E-09) 
---
>   (  1, 1.001070E-06) (  2, 6.313221E-09) (  3, 4.022999E+03) (  4, 6.684358E-09) (  5, 6.498790E-09) (  6, 6.313221E-09) 
cth_lrg_0288.o5580:++++ SSP-07 ++++ cth_large_0288 walltime: 4073 seconds
cth_lrg_0288.o5694:++++ SSP-07 ++++ cth_large_0288 walltime: 4076 seconds

 -------------------------------------

below old for guidance
six minutes later
qhold 5596  Wed Mar 24 09:25:03 CDT 2010

1% du -sm *
10249   5596.pbs.CK
28963   5596.pbs.CK.old

 finished holding at Wed Mar 24 09:28:34 CDT 2010

qstat elapsed time 19 minutes (including file writing time)

1%
28918   5596.pbs.CK
1       cprerror.log
second qrls 5596 resulted in state "Q"
above old for guidance


cd /work/PBS/cpr
du -sm *

date ; qhold 5723 ; date

CTH 64 processes  about 5 minutes
Note the two environment variables is
qsub -v $CPR_ENV,SSPROOT=${WORKDIR}/CTH/wtCPR2/ssp_test cth_standard_0064.bat

increase stopped at
 du -sm *
26663   5723.pbs.CK

% date ; qhold 5723 ; date
Mon May 17 09:32:13 CDT 2010

did not return from qhold and job is still running, in state "R"

explain the previouly gave error and continued to run if time needed
to write directory was more than 5 minutes.

% date ; qhold 5724 ; date

% date ; qhold 5724 ; date
Mon May 17 10:34:13 CDT 2010
Mon May 17 10:37:00 CDT 2010
 288 processes needed less time
For 288 processes file size was
55041   5724.pbs.CK
went into hold mode and qrls released it.


don't do second qhold until all first cases are done
need to say when problems were reported, e.g. 5 minute write limit

Put into appendix.
Send to Chris and Paul Adams.

On March 9, 2010 it was discovered that if the time needed to write
the checkpoint files was more than five minutes then the qhold command
returns an error message and the job does not enter the hold state "H"
but rather continues running to completion.  The ERDC Cray Support Staff
was informed the day this problem was discovered.  On March 19 I was
informed that the parameter causing the 5-minute time limit was
temporarily set to unlimited while we continue testing.  After
some other glitches in the PBS or CPR system, I informed the
ERDC Cray Support Staff on March 25 that the problem had not been
fixed and the jobs did not go into a hold state if the time needed
to write the CPR information was more than five minutes.  In these
cases the jobs gave correct results and the CPR files were cleaned-up
automatically at the end of the jobs.

During the last week of testing, on May 17, the qhold command showed
a different behavior.  The qhold command never returned a prompt.
The CPR files were written but the jobs never went into a hold state,
but rather ran to completion.

Need to check validity
/work/scheinin/CTH/wtCPR2/ssp_test/ded/cth/large/0288
with wtCPR1 and noCPR.

oocore/standard/0064

% date ; qhold 5725 ; date
Mon May 17 11:52:20 CDT 2010
Mon May 17 11:54:25 CDT 2010
% du -sm * ; date
43351   5725.pbs.CK
is on hold

oocore/large/0288
% date ; qhold 5726 ; date
Mon May 17 12:33:02 CDT 2010
Mon May 17 12:35:04 CDT 2010

% du -sm * ; date
34955   5726.pbs.CK
1       cprerror.log
Mon May 17 12:36:23 CDT 2010


hycom/standard/0059
 date ; qhold 5727 ; date
Mon May 17 13:23:12 CDT 2010
Mon May 17 13:24:32 CDT 2010
 du -sm *
26239   5727.pbs.CK


hycom/large/0256
 date ; qhold 5728 ; date
% date ; qhold 5728 ; date
Mon May 17 13:56:54 CDT 2010
Mon May 17 14:01:37 CDT 2010
% du -sm * ; date
93783   5728.pbs.CK


/work/scheinin/WRF/wtCPR2/WRFV3.1.1_bench/wrf/standard/00256
Mon May 17 15:52:51 CDT 2010
Mon May 17 15:54:06 CDT 2010
24205   5729.pbs.CK

/work/scheinin/SSP/wtCPR2/ded/hycom/standard/0059
1% date ; qhold 5730 ; date
Mon May 17 16:20:37 CDT 2010
Mon May 17 16:21:58 CDT 2010
 qrls 5730
scheinin:onyx01% qstat -u scheinin
pbs: 
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5730.pbs        scheinin standard hycom_std_  28017   1   1    --  01:00 R 00:06
Wait 5 minutes
qstat -u scheinin
5730.pbs        scheinin standard hycom_std_  28017   1   1    --  01:00 R 00:11

scheinin:onyx01% qstat -u scheinin

pbs: 
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5730.pbs        scheinin standard hycom_std_  28017   1   1    --  01:00 R 00:11
scheinin:onyx01% date ; qhold 5730 ; date
Mon May 17 16:29:30 CDT 2010
qhold: No such file or directory 5730.pbs
Mon May 17 16:29:30 CDT 2010

scheinin:onyx01% du -sm * 
1       5730.pbs.CK
26084   5730.pbs.CK.old
1       cprerror.log

1% qstat -u scheinin

pbs: 
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
5730.pbs        scheinin standard hycom_std_  28017   1   1    --  01:00 R 00:13

------------------------
qsub -c c=3 -v $CPR_ENV smalltoo.pbs

no checkpoint job written /work/PBS/cpr
(did job fail?)

run to full time without checkpointing to see if it runs OK
run and to qhold qrls to see if that works

with -c c=3 ran for 7 minutes, did not write a checkpoint file

  -------------

Old notes from Marty
Notes:

  - The qhold sends a sigkill to the running job processes 
    after the checkpoint is complete
  - The checkpoint files do not include mmap()ed files or 
    the executable
  - Traced processes (ptrace) cannot be reliably checkpointed
  - The checkpoint/restart options are configurable for 
    cr_checkpoint and cr_restart
  - The -c option may also be used to specify that the job 
    should checkpoint automatically at regulary intervals
    (see qsub man page)
  - The checkpoint files remain in /work/PBS/cpr until the 
    job exits.
  - Did not test node placement requirements for restart
----------------


You can't just take a pre-compiled statically linked app and checkpoint it.  You have to load the blcr module and relink/recompile.  This will link in the necessary library for Checkpoint support.  Passing the LD_PRELOAD env to the job enables the necessary support on the compute nodes.

NOTE: The application MUST be compiled with Cray MPT.

This is not an out-of-the-box implementation of BLCR.  There are significant modifications in ALPS and MPT to enable the support and functionality on XT systems.

Let me know if this is not what you were referencing or if you have any problems.

--Marty

-----------------

After submitting a job, you can look at the 'qstat -f' output to see if checkpointable=yes.

I don't see checkpointable, but I see
 Checkpoint = w=3

---------
with w rather than c file was written to /work/PBS/cp/5740.pbs.CK
---------------

Alan,

I believe that is still the case.  I did tweak the checkpoint scripts somewhat to help ensure that they didn't exit prematurely and completed the ALPS cleanup routine, but I also added a cron job to look for "orphaned" job reservations in ALPS that had no associated PBS job.  In the latter case, the cron job will cancel the leftover reservations and release the resources.

--M.

 ---------------------

Alan,

These are very good questions.  I did a little searching in the PBS UG and found nothing about why one should checkpoint a job or what the feature's intended purpose was.

I'll share my personal opinions with you:

As an administrator, I would say that its purpose is most likely twofold in that it allows jobs to be  recovered after a major system failure and allows jobs to be saved and restarted if the system has to be shut down or the job resources are preempted for other reasons.

I would think that the goal of saving the computational progress of individual jobs to restart in the event of an application failure would be better handled through application level checkpoints or writing restart files at regular intervals.

Unfortunately, I have not had time to really test the impacts of sytsem level events.  I would expect that jobs using a regular checkpoint interval should be restarted after a critical or total system failure.  Alternatively, if the system required a shutdown, the jobs would be checkpointed as the services were stopped and restarted when the system was returned to production.  The only problem would be where Lustre is involved, since that is the filesystem we would be checkpointing to.  In that case, we would probably need to ensure that the PBS daemons were either totally unreachable or were killed with sigkill before initiating the shutdown (in order to simulate a hard system failure to PBS).

These are things I really wish I had time to test.  I know a lot of work was done with CPR at NERSC using MOAB/Torque, but I don't know if they fully embraced it in a production environment.  I think it has a lot of promise and is a feature of past systems that is strongly desired by many in HPC, but I'm not sure of the level of interest or willingness to adequately support it from the vendors (both Altair and HPC).

So, that's probably not worth much, but I at least wanted to get some level of response to you before I left for the day.

I will be out next week with very little or no cell phone or e-mail access.  I'll try to pass some notes on to Jeremy before I leave.

Thanks for your continued efforts.

--Marty

 ---------------------

Alan,

I'm sure those discussions on the Beowulf mailing list were quite colorful, to say the least. :-)

I do not disagree that there should be some simple method to allow user-level checkpoints and it's entirely possible that the process of using a qhold on the job may suffice in providing that functionality.  While the administrator point of view is entirely valid concerning the I/O impact of such features, the reality is that it is almost entirely unavoidable.  If a user chooses to write out everything in memory on every node of his job, there is little an administrator can do to prevent him from doing so.  This, of course, assumes that the user is utilizing all of the memory on the node thus the difference between a full application checkpoint and a full system-level checkpoint would  be limited to the difference in the size of the kernel and system processes which would be fairly minimal.

The difficulty in providing a simple tool or method for this is in the implementation.  At the system level, it requires privileged accounts in order to be able to read all of the node memory and write it to disk.  In doing so, the location of the data and ability to manage resources within the systems internal scheduler and the WMS also require privileged account access.  The result is that the configuration of the CPR capabilities must be tightly controlled and somewhat limited in scope.

Since PBS simply hands off most of the work to a custom "handler" program or script, it may be possible to eventually adapt the process to more user-friendly configurations.  That being said, I think it would be more feasable for PBS (or whatever WMS is in use) to include a future feature for user-level checkpoints.

The real concern, from a system-level perspective, lies in situations where the entire system (or a very large portion of it) needs to be checkpointed all at once.  For large scale systems, the I/O demand would be extreme and could potentially overwhelm the systems internal network and I/O subsystem.  There would have to be very careful planning and testing to ensure precise checkpoint file placement and available capacity on the filesystem.  With distributed filesystems such as lustre, there is a valid concern over allowing checkpoing files to span multiple filesystem segments.  As a result, the recommended solution is to set the checkpoint directories to a stripe width of just one OST.  We mimimize the impact of that by allowing each checkpoint directory to span individual OSTs (setting the OST index to -1), but the impact of over 2100 nodes suddenly writing 8GB files to 160 OSTs all at once (using jade as an example) is more than a little unsettling.  I would love to be able to say, "Sure, no problem, we can handle it." But, in reality, the impact of such an event is unknown (at least to me) and that is not the type of I/O demand the filesystem was designed to handle.  It would require some fairly extensive testing to determine the full impact.

My personal opinion (of which I seem to be sharing freely these days) is that the adoption of CPR within the program will be more suitable to individual job checkpoints (either by the user or system) for use primarily in smaller, longer running jobs and in situations where job preemption may be necessary to push through higher priority, massively parallel jobs.  But, that's just my opinion. :-)

I sincerely hope I haven't rambled too much.  I'm about to head out for the weekend and will be on vacation next week.  I've seen Jeremy the "All you need to know about CPR in 30 seconds or less" guide, so hopefully he'll be able to help out if you run into any issues.

Take care and good luck!

--Marty

 ------------
Alan,

I was going through some online notes and found a reference to an issue that could possibly be related to our "double-checkpoint" problem.

There were no details, but the solution seemed worth a try.

The solution to the problem was to use the "-b" flag to aprun in the job script.  This would prevent the executable from being copied to the compute node for execution (as long as the path and filesystem were available to the compute node - which in our case, they are).

I just thought I'd pass this along as something might want to try.

Take care,

--Marty

 ------------
CPR_ENV=`/usr/bin/cr_run env | grep LD_PRELOAD`
Above does not work if CPR_ENV is already defined because
result is
LD_PRELOAD=/usr/lib64/libcr_run.so.0
CPR_ENV=LD_PRELOAD=/usr/lib64/libcr_run.so.0

/usr/bin/cr_run env | grep LD_PRELOA

