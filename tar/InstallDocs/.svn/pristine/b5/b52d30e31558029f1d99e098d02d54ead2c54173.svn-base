
I have tested three parallel compression programs

  pigz  pbzip2  mpibzip2

pigz
----

The small amount of documentation available says

 > A parallel implementation of gzip for modern
 > multi-processor, multi-core machines

The program was written by Mark Adler.
The biography of Dr. Adler in Wikipedia begins with
 > Dr. Mark Adler (b. April 3, 1959) is best known for his
 > work in the field of data compression.

I was given the binaries to test.  When using aprun to
launch a multi-processor job there is the error message

 > pigz abort: write error on /work/scheinin/ZIP/Data/ran10.orig.gz

The source code page (http://www.zlib.net/pigz/) says
 > To compile and use pigz, please read the README file in the
 > source code distribution. 

so I downloaded the source, but the README file does NOT provide
any useful information on running the program, aside from saying to
type "pigz" to read the command line options.

On one node on jade, the speedup is a little less than a factor of
two.  For one large file, the time needed by serial gzip was 18.6 sec;
for pigz using 2 or 4 threads was 10.9 and 10.8 sec, respectively;
for 8 threads the time was 11.5 sec.

pbzip2
------

This program only uses shared memory and (like pigz) the best
speedup was only a factor of two on jade.  First, the good news.
The program pbzip2 gave no errors when files were compressed with
pbzip2 and decompressed with bunzip2.  Compressing with bzip2 and
decompressing with pbzip2 was also tested, as was the pure pbzip2
case also with no errors.
Correctness was assessed by comparing against the original the md5sum
on files after a cycle of compression and decompression.

Now for the not-so-good news.
The pbzip2 uses shared memory and not MPI.  The value of OMP_NUM_THREADS
has no effect on the number of threads, the option -p# must be used,
where "#" is a number.  File sizes of 37 MB, 170 MB and 1.48 GB were
tested.  The choice of number of threads did not have an effect beyond 2.
On jade, -p2 was approximately a factor of 1.8 faster than -p1, but the
options -p4 and -p8 were slightly slower than -p2.


mpibzip2
--------

The verbose mode displays an important message:

> MPI BZIP2 v0.6  -  by: Jeff Gilchrist [http://compression.ca]
> [Jul. 18, 2007]        (uses libbzip2  by Julian Seward)
> ** This is a BETA version - Use at your own risk! **

The program is a beta version that has not been changed since mid-2007.

Two file sizes were tested, 25 MB and 1.48 GB.
First the bad news, then the good news.
If a file is compressed serially with bzip2,
then a parallel decompression using mpibzip2 -d results in a program error

> *WARNING: Compressed block size is large [1600736346 bytes].
>           If program aborts, use regular BZIP2 to decompress.
> aborting job:

leaving a zero-size file with the name of what should have
been the uncompressed file.

Otherwise, the program mpibzip2 works correctly,
based on the md5sums after a compression/decompression cycle.
Most importantly, it is the only program of the three that shows
good scaling beyond a factor of two.

Various compression and decompression modes were tested.
For parallel compression, the number of processes ranged from
 2 to 16, so even compression with two nodes was tested.
In some cases, serial "bzip2 -d" was used for decompression.
In addition, a file compressed with 8 parallel processes was
decompressed with 2 parallel processes and a file compressed
with 4 parallel processes was decompressed with 8 parallel processes.

On jade:
    Serial compression: 611 seconds.

    # CPUs: 1 Master, 1 Slaves
    Wall Clock: 530.283365 seconds

    # CPUs: 1 Master, 3 Slaves
    Wall Clock: 210.859952 seconds

    # CPUs: 1 Master, 7 Slaves
    Wall Clock: 94.533660 seconds

    # CPUs: 1 Master, 15 Slaves
    Wall Clock: 45.698141 seconds

As we see, scaling was good even when more than one node was used.
The the same file, the OpenMP program, pbzip2 had a wallclock time
of around 352.742787 for two CPUs and did not do better when it was
told that 4 or 8 CPUs were available.  We see that the MPI version is
not impressive when only two processes are used, but it does scale
nicely when more processes are used.  The good scaling with 4 or 8
processes on the same node indicates that the OpenMP version is not
limited by a memory bottleneck.


