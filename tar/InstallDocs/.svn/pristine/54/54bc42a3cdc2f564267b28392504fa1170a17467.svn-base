
Intel Trace Analyzer
====================

Introduction
------------

On the machine diamond, the Intel Trace Analyzer (ITA) provides a
convenient way to monitor application MPI activities gathered by
the Intel Trace Collector (ITC) through graphical displays.
The ITC produces tracefiles that can be analyzed with the ITA.
It was formerly known as Vampirtrace.  As of December 2010, the
most recent installed version is 7.2.1.008.  ITA documentation
files in HTML and PDF format is are in /opt/intel/itac/7.2.1.008/doc
The user will need to transfer the files to his or her workstation
in order to read the documentation.

In practice, even for small, simple MPI programs that ran for less
than a minute, the traces were very large and the traceanalyzer
utility needed more memory than available.  On a Cray computer,
jade, I was able to trace a large program that ran for an hour
using CrayPat and TAU.  For traceanalyzer on diamond even a
small program that ran for a minute caused a segmentation fault,
probably due to insufficient memory.  It is recommended that 
traceanalyzer be run as an interactive PBS job on a compute node
in order to maximize the amount of available memory.  Despite
the usage limitation due to inefficient memory management during
the analysis of the trace data, here are some notes on how to use
ITA and ITC.

Use of ITC for tracing
----------------------

For tracing MPI calls, the trace collector is oriented towards
Intel MPI, which is not the default MPI.  The default MPI is SGI
MPI, for example, module mpi/sgi_mpi-1.26 .  To use Intel MPI
it is necessary to swap modules using the command, for example,

module swap mpi/sgi_mpi-1.26 mpi/intelmpi-4.0.0

(One can see in the makefile GNUmakefile in the
/opt/intel/itac/7.2.1.008/examples/ directory that
VT_MPI == mpt (the SGI MPI) is an option but the environment
setup script will declare the "mpt" is not a supported MPI.)

Instrumentation can be switched on or off at runtime.  Programs need
to be linked with the ITC profiling library.  To define and trace
user-defined events, calls to the ITC API have to be inserted into
the application's source code.  Fortunately, for basic MPI tracing
the user source code does not need to be modified.

As a first step, sourcing the correct file in a shell,

 . <installdir>/bin/itacvars.sh

for shells with the Bourne syntax (sh, ksh, bash) or

source <installdir>/bin/itacvars.csh

for shells with csh syntax (csh, tcsh), will update
LD_LIBRARY_PATH and PATH environment variables as well as set
additional environment variables.
As of December 2010, the <installdir> with the most recent version
of ITAC is /opt/intel/itac/7.2.1.008 .

Information concerning how to modify a program for tracing user-defined
regions or events will not be described in the brief introduction.
That information can be found in the ITC Reference Guide.  Source
files without calls to the ITC API can be compiled with the usual
methods.

Examples provided by Intel can be found in
/opt/intel/itac/7.2.1.008/examples/
It should be noted that the examples show the launching
of an Intel MPI job using "mpiexec" whereas on diamond
a parallel (Intel MPI) executable is launched using "mpirun".
Basic information about running parallel jobs using Intel MPI
can be found in the directory
/usr/local/Example_Codes/Compile_Info_Using_Intels_MPI

For tracing, VT_initialize() and VT_finalize() need to be called
by each process.  For MPI applications, it is not necessary to
call these routines explicitly because it is sufficient for the
program to call MPI_Init() and MPI_Finalize().

A typical Makefile is shown below.

CC        = mpiicc
F77       = mpiifort
IFLAGS    = -I$(VT_ROOT)/include -I.
LFLAGS    = -L$(VT_LIB_DIR)
CFLAGS    = -g  -tcollect
FFLAGS    = -g  
LIBS      = $(MPI_LIB) -lVT $(VT_ADD_LIBS)
CLDFLAGS  = $(LFLAGS) $(TLIB) $(LIBS)
FLDFLAGS  = $(LFLAGS) $(TLIB) $(LIBS)

all: sor.exe

sor.exe: sor.o
	$(CC) $+ $(CLDFLAGS) -o $@

# sor.o: sor.c

# build rules for object files
%.o: %.f
	$(F77) $(IFLAGS) $(FFLAGS) -c $<

%.o: %.c
	$(CC) $(IFLAGS) $(CPPFLAGS) $(CFLAGS) -c $<

# disable implicit rule for building executables
%: %.o
%: %.c
%: %.f

clean:
	-rm -f *.o *.exe

A typical Makefile is shown above.

Note that for the Intel compiler, tracing is enabled by
using the "-tcollect" option.

Runtime control of tracing
--------------------------

The environment variable VT_CONFIG contains the pathname of an
ITC configuration file to be read at MPI initialization time.
What information is traced is determined at the start of the
program execution rather than when the executable is built.

It is possible to gather statistics without collecting trace data.
Either set the follow environment variables
VT_STATISTICS=ON
VT_PROCESS=OFF
or in the VT_CONFIG file have the following lines.
STATISTICS ON
PROCESS 0:N OFF
The letter "N" is a symbol for the maximum rank.

Statistics are written into the STF file.  Conversion from a
machine-readable for to ASCII text can be done with
stftool --print-statistics

To record the location of subroutine calls in the source code,
the compilation should be done in the debug mode (option -g).

To enable Program Counter (PC) tracing, at run time set the
environment variable
VT_PCTRACE=<num>
where <num> is the number of call levels, for example, 5.
An alternative is to have the following lines in the VT_CONFIG file.
ACTIVITY MPI <num>
PCTRACE ON
which will trace one call level on all routines not
mentioned explicitly.
In order to trace 5 levels deep, the syntax would be
PCTRACE 5

The VT_CONFIG file can filter which routines are traced.
All matching is case-insensitive.  Matching is done in
the same order as the lines in the file.  For example,
# disable all MPI activities
ACTIVITY MPI OFF
# enable all MPI send routines
STATE MPI:*send ON
# except MPI_Bsend
SYMBOL MPI_bsend OFF

The file that ends with the suffix ".prot" is known as the "protocol
file".  It lists all options with their values used when the program
was started.  This file may indicate some options that are available
though not used.  The protocol file contents can be used as the
VT_CONFIG file in order to reproduce exactly a particular test.

The ITC Reference Guide explains how to obtain traces from an MPI
program that fails abruptly.  Various advanced tracing options and
techniques can be found in the reference guide and are not described
in this brief summary.  Also in the reference guide is a description
of how to use PAPI if it has been installed.

Using the trace analyzer
------------------------

After execution of the user's program, a large number of trace
files are generated.  The file <executable>.stf, where <executable>
is the name of the program executable that was traced, indicates
to traceanalyzer the names of the other trace files associated with
the run.  To launch the Graphical User Interface version of the Intel
Trace Analyzer and Collector, enter "traceanalyzer" at the command line:

 <installdir>/bin/traceanalyzer <STF file>

where <STF file> will have as default the name <executable>.stf
Assuming itacvars.sh or itacvars.csh has already been sourced,
the command can be simplified to

traceanalyzer <STF file>

Because of the immense usage of virtual memory by traceanalyzer,
it is strongly recommended that it be run on a compute node.
There is a script, xdiamond, for starting an interactive PBS job
that allows remote display on your workstation of the graphical
display of traceanalyzer.  The command is

/usr/local/usp/apptools/xdiamond -np 8 -A <account> -walltime 2 -q standard

where <account> is the users project identifier and the "-walltime" has
units of hours.  The "2" in the above command is just an example.
(The project identifier can be found using the "show_usage" command.)
When the interactive PBS job begins, the user needs to source a
particular file using the command

. /tmp/X_PBS_ENV

(Notice the period "." before the file name.)  Then the user needs
to setup the environment using

module swap mpi/sgi_mpi-1.26 mpi/intelmpi-4.0.0
export VT_ROOT=/opt/intel/itac/7.2.1.008
source $VT_ROOT/bin/itacvars.sh

The exact version numbers in the above commands will change over time.

Once an interactive PBS job begins, the user can log-into the compute
node from another session on diamond in order to understand the
amount of virtual memory that is used when the "traceanalyzer <STF file>"
command is invoked, using the "top" command.  During the initial
phase of running traceanalyzer, if the amount of virtual memory
approaches or exceeds 20 GB, it may be necessary to redo the run
and reduce the amount of trace data that is generated.  If it is
possible to obtain useful information while only tracing a subset
of processes, the VT_CONFIG file could be constructed as shown below
in the batch job before launching the executable.

export VT_CONFIG=vt.config
cat <<here > $VT_CONFIG
PROCESS 0:N OFF
PROCESS 0:15 ON
PCTRACE ON
here

The above example would only trace the first 16 processes.  The
number "15" shown above could be changed as needed.  Experiments
on a simple program indicate that the sizes of the largest trace
files are roughly proportional to the number of processes being
traced.  Documentation on the use of traceanalyzer can be found in
<installdir>/doc/ITA_Reference_Guide.pdf .  This writeup will only
summarize some useful features.

Event Timeline.  After starting traceanalyzer, click on the "Charts"
tab then choose "Event Timeline".  A timeline of routine calls will
be shown.  In addition to routine calls, lines will show the MPI
communication between processes.  Moving the mouse over a section
with the left mouse button down will zoom-in on a time interval.
Right-clicking a section named MPI on the timeline and choosing
"Ungroup Group MPI" will show individual MPI calls.

Message Profile.  A matrix of which process communicates with which
process can be viewed by clicking on the tab "Charts" then choosing
"Message Profile".  A similar chart for collection communications
can be generated by choosing "Collective Operations Profile".

Navigate Menu.  The tab "Navigate" can be used to change the time
interval being displayed.  The Navigate panel also shows the keyboard
options for navigating.  For example, "I" and "O" will zoo-in and
zoom-out with respect to the time interval.

Function Group Editor.  The bottom of the display allows the user to
select the processes and to the right of the there is a button
to select the routines display.  The button does not have a fixed
label, but rather, the label shows the mode of operation.  Of
particular interest is the choice (made available by left-clicking
on the button) of "All Functions" which shows the user routines
as well as MPI routines.  The user routines can be useful for
placing the MPI calls into context.

Filter.  At the bottom of the display, at the end, the "Filter"
button can be found.  Right clicking, then choosing "Functions" then
"Custom" will provide a pull-down menu that can be used to select
specific functions.  The timeline can be easier to read when only
a few selected functions are displayed rather than showing all routines.

Call Tree.  Near the middle of the display (for the default layout) there
is a button called "Call Tree" that shows the call tree and the times
spent at each level.  What is actually shown depends on the groups
(second button on bottom) as well as the "Filter" button at the bottom
of the display.

Load Balance.  Right-clicking on "Load Balance" (next to "Call Tree" on
the middle menu bar) will show the groups and clicking on the box with
the plus sign to expland the group will show the total time spent
with each process.

Display Colors.  Left-clicking on the Function Group Editor (in the
menu bar at the bottom of the display) then right-clicking on a
particular function will display a menu that allows the user to select
the color used to display the function.  This can be very useful when
preparing documentation intended to highlight specific functions,
since the number of user routines and MPI routines typically far
exceeds the number of usefully distinct colors.

Saving.  On the top menu bar, under "View" there is the option "Save"
that can be used to save the entire display in any one of several
pixel-based formats.  The result can be viewed and formated for printing
by using the command "display" which is an Image Magick application
available on most Linux workstations.

Many more options are described in the ITA Reference Guide.  This
overview may serve as a quick-start guide and serves to indicate the
types of analysis available using traceanalyzer.
