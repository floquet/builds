\documentclass[12pt]{article}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry} 
\usepackage{mystuff}

\begin{document}

\setcounter{section}{0}

\title{ MPI-IO Special Emphasis Project}

\date{Sept. 30, 2009}
\author{Alan L. Scheinine\\
 Lockheed-Martin, ERDC-ITL-MS}
\maketitle

\section{As-Is Assessment}

The first component of the project has three tasks,
whose results are described in this report.

\subsection{Document and assess state of support for existing vendor tools for MPI-IO support by machine and center}

The web sites for the various centers have very little information.
I looked at the index of the Users Manual for various machines
and found that almost nothing is written about I/O optimization
except the advice to use \$WORKDIR rather than \$HOME.
Each site had a search engine and I used as search terms ``MPI IO'',
``parallel filesystem'' \& ``parallel IO'' and
also just ``filesystem'' \& ``MPI''; yet found close to nothing about
parallel I/O.
The ERDC site had the most hits but still not much.
Also, I looked at directories on various machines that might
contain documentation.
I found nothing about parallel I/O except at ERDC there was in
\texttt{/usr/local/Example\_Codes/}
 a subdirectory that explains why MPI-IO
 is necessary for contemporaneous writes.
Also there is a subdirectory on OST stripes.

I only found at ERDC information that the striping can be
modified by the user.
Even so, I did not find information
about what amount of striping that is efficient
for various ways of using MPI-IO.

On the other hand, technical representatives from Cray and SGI indicated
to me manuals that describe routines written especially for
efficient use of parallel file systems \cite{docs-cray.S-0013-10,
docs-sgi.007-4639-010}.
It may be unfair to say that the documentation does not exist
at the sites,
because perhaps the system administrators at the sites could
indicate the appropriate documentation.
In any case, an argument can be made for providing a parallel I/O
documentation package at each site that is easily accessable.

\subsection{Document existing 3rd party libraries/tools for parallel I/O by machine and center}

The vendor documentation on parallel I/O emphasizes using
Parallel HDF5 and Parallel NetCDF.
Indeed, software such as WRF
uses parallel I/O by way of Parallel NetCDF.
Parallel NetCDF is in the Consolidated Software listing so one
would expect many sites to have it.
Moreover, Parallel HDF5 is in the Consolidated Software listing.
In addition, parallel HDF5 is part of Cray's SCILIB.
Looking at specific machines, I found
\begin{verbatim}
 mjm under /usr/cta/pet/pkgs/ has
    netcdf-4.0.1-parallel
       linked with hdf5-1.8.2-parallel
    hdf5-1.8.3-pgi-parallel

 sapphire under /usr/local/usp/PETtools/CE/pkgs has
    hdf5-1.8.3 and hdf5-1.8.3-serial
    netcdf-4.0.1 and netcdf-4.0.1-serial
    so one might assume that the non-serial hierarchies are parallel.

 einstein under /usr/local/PET/pkgs/ has
    hdf5-1.8.3-parallel
    netcdf-4.0.1 (not labelled as parallel but nc-config shows that
            it is linked with hdf5-1.8.2-parallel

 pingo under /usr/local/pkg/ has
    netcdf/netcdf-4.0.gnu.snl but this version does not have a file
    that describes how it was linked and the contents of libnetcdf.a
    (using nm) barely makes mention of hdf5.

 jaws-login1.mhpcc.hpc.mil
    Jaws is going away but I cannot log into mana.  They have a
    software list on their web site that says it has HDF5 and NetCDF
    but the web pages do not say what directory and I cannot find the
    software when logging-into jaws.
\end{verbatim}

No doubt the problem of finding the software on jaws could be
solved quickly by writing to the local helpdesk.
In any case, it is interesting that the PET software has different
versions on the various machines.
It would be valuable to contact the people directly involved
in installing and using the parallel I/O HDF5 and NetCDF in
order to know whether testing was done to optimize the software
with regard to striping and block size.

\subsection{Document significant community libraries/tools for parallel I/O (ADIOS, etc.)}

Bob Ross of ANL wrote a useful description of practical
parallel I/O  \cite{Ross:CScADS.leadershipcomputing.2009}.
In particular he lists parallel HDF5, parallel NetCDF and ADIOS.
All DOD sites have HDF5 and NetCDF but documentation is lacking.
There is not just a lack of documentation concerning parallel
HDF5 and NetCDF, but also, if MPI-IO is used
then environmental variables could be set for tuning MPI-IO,
but this is not documented.  Also, there is little documentation
as to whether disk striping can be modified by the user.

I would propose that adding URL links to documentation is not enough.
For example, in theory the I/O in the program HYCOM has been optimized
but in practice parallel I/O in HYCOM is not efficient.  So the actual
effort of getting a set of programs to have fast parallel I/O at various
computer centers should result in practical notes that can complement
the standard manuals provided by the vendors.

ADIOS is not in the Consolidated Software listing.
Information on ADIOS can be found in \cite{10.1109/IPDPS.2009.5161052,
Jin:CUG.2008}.
There is a need to find program developers who would consider ADIOS
to be useful and for whom the staff of the Special Emphasis Project
could provide some programming and some benchmarking support.

\subsection*{Discussion}

It would be very useful to find people who have already looked
into using parallel I/O for several reasons:
\begin{itemize}
\item In order to discover what information is available (where to look,
who to ask)
\item To discover how much needs to be done,
e.g.\ whether it would be useful to benchmark the performance
over a wide range of parameters or whether that has already been
done by someone
\item In order to define projects that are more
substantial than simply adding links on web pages to manuals
or white papers provided by the vendors.
\end{itemize}

\subsection*{Details on Parallel I/O Documentation at Sites}

This subsection contains further details about the documentation
on parallel I/O that I found at the various DoD HPC sites.

\subsubsection*{\underline{Web pages}}

Searched using ``MPI IO'' or ``parallel filesystem'' or ``parallel IO''
and also simply ``filesystem'' or ``MPI''.  Also, looked at the topics
of various user manuals.

\begin{description}
\item[www.afrl.hpc.mil]
      They say to use \$WORKDIR rather than \$HOME.
      I could not find any other documentation about
      parallel IO at this site.
\item[www.arsc.edu]
      They say to use \$WORKDIR rather than \$HOME.
      I could not find any other documentation about
      parallel IO at this site.
\item[www.arl.hpc.mil]
      It appears in one user manual that they hve no parallel file
      system.  Their basic `new user' documentation indicates that they
      only have NFS and local (to node) store, however the MJM
      User Guide says that /usr/var/tmp is a GPFS parallel file system.
      Their Altix ICE 8200 (e.g. Harold) will have Lustre.
\item[www.navo.hpc.mil]
      The GPFS on davinci is described as ``extremely fast and
      extremely large''.  As with other systems, the specs do
      not give information about the filesystem performance,
      just the size.
\item[www.mhpcc.hpc.mil]
      I found very little information on anything.

\item[www.erdc.hpc.mil]
      The /work filesystem has the highest file transfer rates.
      (Under Tips and Tricks and Users Guide one finds information
      on Integer Sizes for MPI-IO Parameters OFFSET and DISPLACEMENT.
      However, the information relates to avoiding an error rather
      than efficiency.)
\end{description}

\subsubsection*{\underline{What I found under /usr/local after logging-into a front-end node}}

\begin{description}
\item[sapphire.erdc.hpc.mil]
      \texttt{/usr/local/Example\_Codes/MPI-IO\_Example}
      explains that direct access I/O on the Cray XT fails if more than one
      process write to the file and that the solution is to use MPI-IO.
      \texttt{/usr/local/Example\_Codes/OST\_Stripes}
      explains that the /work file system has 160 OSTs and that the
      default is stripping over two OSTs.  The example shows how to
      set striping to use more OSTs on a file or a directory.
\item[harold.arl.hpc.mil]
      The new SGI machines are not yet available though harold and tow
      are described on the ARL web pages.
\item[mjm-l2.arl.hpc.mil]  Did not find an example directory
\item[hawk.afrl.hpc.mil]  Nothing in /usr/local
\item[einstein.navo.hpc.mil] Has only pbs in examples:
                      \texttt{/usr/local/Example\_Codes/pbs\_scripts/}
\item[pingo.arsc.edu]  Nothing in /usr/local
\item[davinci.navo.hpc.mil]  Nothing in /usr/local
\item[jaws-login1.mhpcc.hpc.mil]  Nothing in /usr/local or /local
\end{description}

\subsubsection*{\underline{System News}}

On the machine midnight at ARSC there is a news item about striping\\
\verb|http://www.arsc.edu/support/news/systemnews/midnightnews.xml#file_striping|\\
On the other hand, MPI-IO is not available on midnight.

\bibliographystyle{unsrt}
\bibliography{mpi-io}
\end{document}

